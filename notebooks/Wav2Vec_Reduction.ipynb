{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prsood/projects/def-whkchun/prsood/sarcasm_venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2FeatureExtractor, AutoConfig\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool(input : np.array , mode : str) -> np.array:\n",
    "    \"\"\"\n",
    "    Supported modes are 'mean', 'max' and 'median'\n",
    "    Given an array with one dimension, we take the mean max or\n",
    "    median of it and return it\n",
    "    \"\"\"\n",
    "    if mode == 'mean':\n",
    "        return torch.Tensor(input.mean(0))\n",
    "    elif mode == 'max':\n",
    "        return torch.Tensor(input.max(0))\n",
    "    elif mode == 'median':\n",
    "        return torch.Tensor(np.median(input,0))\n",
    "    else:\n",
    "        raise NotImplementedError(\"The supported modes are 'mean', 'max' and 'median'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path , target_sampling_rate):\n",
    "\n",
    "    # path = path[6:]\n",
    "    \n",
    "    speech_array, sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return pool(speech , \"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36181])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_file_to_array_fn(batch[0] , 16000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_process_values(input_val, processor , target_sampling_rate):\n",
    "    # m = 166571 # takes 8 minutes to get this value on a pre-processing step with 10K data points\n",
    "    m = max(map(np.shape , input_val))[0]\n",
    "    inp = []\n",
    "    for matrix in input_val:\n",
    "        n = matrix.shape[0]\n",
    "        mat = np.pad(matrix, (0, m-n), 'constant')\n",
    "        inp.append(mat)\n",
    "    \n",
    "\n",
    "    result = processor(inp, sampling_rate=target_sampling_rate)\n",
    "\n",
    "    result = result['input_values']\n",
    "\n",
    "    return result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Processor(batch):\n",
    "    model_path = \"facebook/wav2vec2-large-960h\"\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "    target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "    speech_list = []\n",
    "\n",
    "\n",
    "\n",
    "    for (input_path) in batch:\n",
    "        speech_list.append(speech_file_to_array_fn(input_path , target_sampling_rate))\n",
    "\n",
    "\n",
    "    # label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    speech_list = pad_and_process_values(speech_list , processor , target_sampling_rate )\n",
    "    return torch.Tensor(np.array(speech_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36181])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Processor(batch) \n",
    "p[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "       \n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)  \n",
    "        # self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(self, hidden_states, mode=\"mean\"):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None, output_attentions=None, output_hidden_states=None, return_dict=None, labels=None,):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        return self.wav2vec2(input_values,attention_mask=attention_mask,output_attentions=output_attentions,output_hidden_states=output_hidden_states,return_dict=return_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"facebook/wav2vec2-large-960h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForSpeechClassification.from_pretrained( \"facebook/wav2vec2-large-960h\" , config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()\n",
    "model._get_feat_extract_output_lengths(1024).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._get_feat_extract_output_lengths(3280).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureExtractor(batch):    \n",
    "    model_path = \"facebook/wav2vec2-base-100k-voxpopuli\"\n",
    "    feature_extractor =  Wav2Vec2FeatureExtractor.from_pretrained(model_path) \n",
    "    target_sampling_rate = feature_extractor.sampling_rate\n",
    "\n",
    "    speech_list = []\n",
    "\n",
    "    number_of_channels = 6\n",
    "\n",
    "\n",
    "    for (input_path) in batch:\n",
    "        speech_list.append(speech_file_to_array_fn(input_path , target_sampling_rate))\n",
    "\n",
    "\n",
    "    # label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    speech_list = pad_and_process_values(speech_list , number_of_channels , feature_extractor , target_sampling_rate )\n",
    "    return torch.Tensor(np.array(speech_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\"../data/test_splits_wav/dia0_utt0.wav\", \"../data/test_splits_wav/dia1_utt0.wav\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first wav file outputs 217086 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36181])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Processor(batch) \n",
    "p[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([217086])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe = FeatureExtractor(batch)\n",
    "fe[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "if len(p) == len(fe) == len(batch):\n",
    "    print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        \"facebook/wav2vec2-base-100k-voxpopuli\",\n",
    "    )\n",
    "wav2vec2 = Wav2Vec2Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = wav2vec2(fe,attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch\n",
    "output = SequenceClassifierOutput(loss=None, logits=torch.Tensor([[-4.4651e-02,  7.5752e-05, -2.2422e-02, -4.2450e-03,  2.3928e-02,\n",
    "         -2.7536e-02, -4.1361e-02],\n",
    "        [-5.3067e-02,  9.4667e-03, -1.5667e-02, -6.3375e-03,  4.1833e-02,\n",
    "         -1.1422e-02, -4.0160e-02],\n",
    "        [-3.5620e-02,  2.4163e-03, -1.4061e-02,  1.4004e-02,  2.4372e-02,\n",
    "         -2.5579e-02, -4.3301e-02],\n",
    "        [-5.9695e-02, -8.3358e-03, -1.8136e-02, -1.5357e-02,  6.5647e-02,\n",
    "         -3.3532e-02, -5.0336e-02]]), hidden_states=None, attentions=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output.logits , dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0166, -0.0108, -0.0111, -0.0171])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(output.logits, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('sarcasm_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "682255e32c8b7c7832e1c984c03ff3b577376f85199a15f316109068883e84f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
