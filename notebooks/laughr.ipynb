{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_friendly_class(y_class):\n",
    "    if y_class[0] == 1 and y_class[1] == 0:\n",
    "        return 'Laugh'\n",
    "    if y_class[0] == 0 and y_class[1] == 1:\n",
    "        return 'Dialog'\n",
    "\n",
    "class RawClip3(object):\n",
    "    featureFuncs = ['tonnetz', 'spectral_rolloff', 'spectral_contrast',\n",
    "                    'spectral_bandwidth', 'spectral_flatness', 'mfcc',\n",
    "                    'chroma_cqt', 'chroma_cens', 'melspectrogram']\n",
    "\n",
    "    def __init__(self, sourcefile, Y_class=None):\n",
    "        self.y, self.sr = sf.read(sourcefile)\n",
    "        self.laughs = None\n",
    "        self.Y_class = Y_class\n",
    "\n",
    "    def resample(self, rate, channel):\n",
    "        return librosa.resample(self.y.T[channel], self.sr, rate)\n",
    "\n",
    "    def amp(self, rate=22050, n_fft=2048, channel=0):\n",
    "        D = librosa.amplitude_to_db(librosa.magphase(librosa.stft(\n",
    "            self.resample(rate, channel), n_fft=n_fft))[0], ref=np.max)\n",
    "        return D\n",
    "\n",
    "    def _extract_feature(self, func):\n",
    "        method = getattr(librosa.feature, func)\n",
    "\n",
    "        # Construct params for each 'class' of features\n",
    "        params = {'y': self.raw}\n",
    "        if 'mfcc' in func:\n",
    "            params['sr'] = self.sr\n",
    "            params['n_mfcc'] = 128\n",
    "        if 'chroma' in func:\n",
    "            params['sr'] = self.sr\n",
    "\n",
    "        feature = method(**params)\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def _split_features_into_windows(self, data, duration):\n",
    "        # Apply a moving window\n",
    "        windows = []\n",
    "\n",
    "        # Pad the rightmost edge by repeating frames, simplifies stretching\n",
    "        # the model predictions to the original audio later on.\n",
    "        data = np.pad(data, [[0, duration], [0, 0]], mode='edge')\n",
    "        for i in range(data.shape[0] - duration):\n",
    "            windows.append(data[i:i+duration])\n",
    "\n",
    "        return np.array(windows)\n",
    "\n",
    "    def build_features(self, duration=30, milSamplesPerChunk=10):\n",
    "        # Extract features, one chunk at a time (to reduce memory required)\n",
    "        # Tip: about 65 million samples for a normal-length episode\n",
    "        # 10 million samples results in around 1.5GB to 2GB memory use\n",
    "        features = []\n",
    "\n",
    "        chunkLen = milSamplesPerChunk * 1000000\n",
    "        numChunks = math.ceil(self.y.shape[0] / chunkLen)\n",
    "\n",
    "        for i in range(numChunks):\n",
    "            # Set raw to the current chunk, for _extract_feature\n",
    "            self.raw = self.y.T[0][i * chunkLen:(i+1)*chunkLen]\n",
    "\n",
    "            # For this chunk, run all of our feature extraction functions\n",
    "            # Each returned array is in the shape (features, steps)\n",
    "            # Use concatenate to combine (allfeatures, steps)\n",
    "            chunkFeatures = np.concatenate(\n",
    "                list(\n",
    "                    map(self._extract_feature, self.featureFuncs)\n",
    "                    )\n",
    "                )\n",
    "            features.append(chunkFeatures)\n",
    "\n",
    "\n",
    "        # Transform to be consistent with our LSTM expected input\n",
    "        features = np.concatenate(features, axis=1).T\n",
    "        # Combine our chunks along the time-step axis.\n",
    "        features = self._split_features_into_windows(features, duration)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "class DataSet(object):\n",
    "    def __init__(self, datapath, laughPrefix='/ff*.wav', dialogPrefix='/dd*.wav'):\n",
    "        self.clips = []\n",
    "        for y_class, files in [[1., 0.], glob(datapath + laughPrefix)], [[0., 1.], glob(datapath + dialogPrefix)]:\n",
    "            for ff in files:\n",
    "                self.clips.append(RawClip3(ff, y_class))\n",
    "        np.random.seed(seed=0)\n",
    "        self.X, self.Y_class = self._get_samples()\n",
    "        self.idx_train, self.idx_cv, self.idx_test = self.split_examples_index(len(self.Y_class))\n",
    "\n",
    "    def split_examples_index(self, total):\n",
    "        \"\"\"Returns shuffled index for 60/20/20 split of train, cv, test\"\"\"\n",
    "        np.random.seed(seed=0)\n",
    "        idx = np.random.choice(total, size=total, replace=False, )\n",
    "\n",
    "        #60/20/20 split\n",
    "        train = idx[0:int(total*0.6)]\n",
    "        cv    = idx[int(total*0.6):int(total*0.6) + int(total*0.2)]\n",
    "        test  = idx[int(total*0.8):]\n",
    "\n",
    "        return train, cv, test\n",
    "\n",
    "    def _get_samples(self):\n",
    "        X = []\n",
    "        y = []\n",
    "        for clip in self.clips:\n",
    "            for s in clip.build_features():\n",
    "                X.append(s)\n",
    "                y.append(clip.Y_class)\n",
    "                \n",
    "        return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib as plt\n",
    "\n",
    "def predict_graph_clip(filename, model):\n",
    "    # Load the clip, extract features, and run the model\n",
    "    rc2 = RawClip3(filename, Y_class=None)\n",
    "    X = rc2.build_features()\n",
    "    spec = rc2.amp()\n",
    "    classes = model.predict(X)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12,2))\n",
    "    \n",
    "    # Spectra\n",
    "    axes = librosa.display.specshow(spec, y_axis='log', x_axis='frames', cmap='gist_gray')\n",
    "    \n",
    "    # Setup second x and y axes\n",
    "    ax2 = axes.twinx()\n",
    "    ax2.set_ylabel('Prediction')\n",
    "    ax3 = ax2.twiny()\n",
    "    ax3.margins(0,0.1)\n",
    "    \n",
    "    # Plot the laugh class as a line graph\n",
    "    #g = ax3.plot(x2[:,1], linewidth=2, color=[0.8,0.8,0.8])\n",
    "    g = ax3.plot(classes[:,0], linewidth=2, color=[0.0,1.0,1.0])\n",
    "    \n",
    "    # Add a title\n",
    "    plt.xticks([])\n",
    "    plt.title('%s spectra and laugh classification' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('/home/prsood/projects/def-whkchun/prsood/laughr/assets/trained-model.h5')\n",
    "f = \"/home/prsood/projects/def-whkchun/prsood/multi-modal-emotion/data/final_context_videos/1_60_c.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in glob('./S*sample?.wav'):\n",
    "#     predict_graph_clip(f, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_graph_clip(f, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarcasm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
