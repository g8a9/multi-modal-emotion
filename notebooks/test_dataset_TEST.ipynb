{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavsood/Documents/DDI/Gitlab/multi-modal-emotion/src/emotion_venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "DATA_PATH = \"/Users/pranavsood/Documents/DDI/Gitlab/multi-modal-emotion/data\"\n",
    "SAVE_PATH = \"/Users/pranavsood/Documents/DDI/Gitlab/multi-modal-emotion/src/run_scripts/save_files_from_run\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}\n",
    "NEW = {v: k for k, v in ORIG.items()} # reverse orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the dataset and perform some operations to process the dataset into what the code expects later on\n",
    "    \"\"\"\n",
    "    df = pd.read_pickle(dataset)\n",
    "    # print(df)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    **REMOVED DIA125_UTT3 AS ITS NOT IN VAL DATASET**\n",
    "    **REMOVED DIA110_UTT7 AS ITS NOT IN VAL DATASET**\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    folder_name = \"data/\"\n",
    "    df.rename(columns = {'y':'emotion'}, inplace = True)\n",
    "    df['path'] = folder_name + df['split'] + \"_splits_wav/\" + \"dia\" + df['dialog'].astype(str) + '_utt' + df['utterance'].astype(str) + \".wav\"\n",
    "    df['name'] = \"dia\" + df['dialog'].astype(str) + '_utt' + df['utterance'].astype(str) \n",
    "    # df.drop(columns = [\"dialog\" , \"utterance\" , \"text\"  , \"num_words\"] , inplace=True)\n",
    "    df['label'] = df['emotion'].map(NEW)\n",
    "\n",
    "    df = df[ df[\"name\"] != \"dia110_utt7\"] \n",
    "    df = df[ df[\"name\"] != \"dia125_utt3\"] \n",
    "\n",
    "    df.to_pickle(f\"{DATA_PATH}/emotion_pd_raw.pkl\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(f\"{DATA_PATH}/emotion_pd.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_raw = pd.read_pickle(f\"{DATA_PATH}/emotion_pd_raw.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emotion</th>\n      <th>dialog</th>\n      <th>utterance</th>\n      <th>text</th>\n      <th>num_words</th>\n      <th>split</th>\n      <th>path</th>\n      <th>name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>also i was the point person on my company s tr...</td>\n      <td>19</td>\n      <td>train</td>\n      <td>data/train_splits_wav/dia0_utt0.wav</td>\n      <td>dia0_utt0</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>you must ve had your hands full</td>\n      <td>7</td>\n      <td>train</td>\n      <td>data/train_splits_wav/dia0_utt1.wav</td>\n      <td>dia0_utt1</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>that i did that i did</td>\n      <td>6</td>\n      <td>train</td>\n      <td>data/train_splits_wav/dia0_utt2.wav</td>\n      <td>dia0_utt2</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>so let s talk a little bit about your duties</td>\n      <td>10</td>\n      <td>train</td>\n      <td>data/train_splits_wav/dia0_utt3.wav</td>\n      <td>dia0_utt3</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>now you ll be heading a whole division , so yo...</td>\n      <td>17</td>\n      <td>train</td>\n      <td>data/train_splits_wav/dia0_utt5.wav</td>\n      <td>dia0_utt5</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13703</th>\n      <td>6</td>\n      <td>279</td>\n      <td>2</td>\n      <td>you stole them from me !</td>\n      <td>6</td>\n      <td>test</td>\n      <td>data/test_splits_wav/dia279_utt2.wav</td>\n      <td>dia279_utt2</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>13704</th>\n      <td>6</td>\n      <td>279</td>\n      <td>3</td>\n      <td>you stole them from me ! !</td>\n      <td>7</td>\n      <td>test</td>\n      <td>data/test_splits_wav/dia279_utt3.wav</td>\n      <td>dia279_utt3</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>13705</th>\n      <td>6</td>\n      <td>279</td>\n      <td>4</td>\n      <td>gimme them !</td>\n      <td>3</td>\n      <td>test</td>\n      <td>data/test_splits_wav/dia279_utt4.wav</td>\n      <td>dia279_utt4</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>13706</th>\n      <td>6</td>\n      <td>279</td>\n      <td>7</td>\n      <td>look , i really need some help , okay ? why ? ...</td>\n      <td>29</td>\n      <td>test</td>\n      <td>data/test_splits_wav/dia279_utt7.wav</td>\n      <td>dia279_utt7</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>13707</th>\n      <td>6</td>\n      <td>279</td>\n      <td>10</td>\n      <td>yeah , fade the accent out and people will thi...</td>\n      <td>24</td>\n      <td>test</td>\n      <td>data/test_splits_wav/dia279_utt10.wav</td>\n      <td>dia279_utt10</td>\n      <td>anger</td>\n    </tr>\n  </tbody>\n</table>\n<p>13704 rows Ã— 9 columns</p>\n</div>",
      "text/plain": "       emotion dialog utterance  \\\n0            0      0         0   \n1            0      0         1   \n2            0      0         2   \n3            0      0         3   \n4            0      0         5   \n...        ...    ...       ...   \n13703        6    279         2   \n13704        6    279         3   \n13705        6    279         4   \n13706        6    279         7   \n13707        6    279        10   \n\n                                                    text  num_words  split  \\\n0      also i was the point person on my company s tr...         19  train   \n1                        you must ve had your hands full          7  train   \n2                                  that i did that i did          6  train   \n3           so let s talk a little bit about your duties         10  train   \n4      now you ll be heading a whole division , so yo...         17  train   \n...                                                  ...        ...    ...   \n13703                           you stole them from me !          6   test   \n13704                         you stole them from me ! !          7   test   \n13705                                       gimme them !          3   test   \n13706  look , i really need some help , okay ? why ? ...         29   test   \n13707  yeah , fade the accent out and people will thi...         24   test   \n\n                                        path          name    label  \n0        data/train_splits_wav/dia0_utt0.wav     dia0_utt0  neutral  \n1        data/train_splits_wav/dia0_utt1.wav     dia0_utt1  neutral  \n2        data/train_splits_wav/dia0_utt2.wav     dia0_utt2  neutral  \n3        data/train_splits_wav/dia0_utt3.wav     dia0_utt3  neutral  \n4        data/train_splits_wav/dia0_utt5.wav     dia0_utt5  neutral  \n...                                      ...           ...      ...  \n13703   data/test_splits_wav/dia279_utt2.wav   dia279_utt2    anger  \n13704   data/test_splits_wav/dia279_utt3.wav   dia279_utt3    anger  \n13705   data/test_splits_wav/dia279_utt4.wav   dia279_utt4    anger  \n13706   data/test_splits_wav/dia279_utt7.wav   dia279_utt7    anger  \n13707  data/test_splits_wav/dia279_utt10.wav  dia279_utt10    anger  \n\n[13704 rows x 9 columns]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = emotion_raw['emotion'].values \n",
    "        # Want a tensor of all the features d\n",
    "audio_features = emotion_raw['path'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_features[idx] = data/train_splits_wav/dia0_utt3.wav \n",
      "labels[idx] = 0\n"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "print(f\"audio_features[idx] = {audio_features[idx]} \\nlabels[idx] = {labels[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(13704,)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize(x):\n",
    "    return torch.tensor( x, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorizePandas(df):\n",
    "    df['input_values']=df['input_values'].progress_apply(tensorize)\n",
    "    df['labels']=df['labels'].progress_apply(tensorize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2608/2608 [00:05<00:00, 517.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2608/2608 [00:00<00:00, 94114.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_pd = tensorizePandas(pd.read_pickle(f\"{SAVE_PATH}/train_pkl.pkl\"))\n",
    "# test_pd = tensorizePandas(pd.read_pickle(f\"{SAVE_PATH}/test_pkl.pkl\"))\n",
    "val_pd = tensorizePandas(pd.read_pickle(f\"{SAVE_PATH}/val_pkl.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pd = pd.concat([train_pd , test_pd , val_pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pd.to_pickle(f\"{DATA_PATH}/emotion_audio_all_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([tensor([-0.1474, -0.8100, -0.4489,  ...,  0.0037,  0.0014,  0.0014],\n              dtype=torch.float64)                                         ,\n       tensor([ 0.0158, -0.0453, -0.0712,  ..., -0.0026,  0.0034,  0.0019],\n              dtype=torch.float64)                                         ,\n       tensor([-0.0326, -0.0466, -0.0405,  ..., -0.0003, -0.0003, -0.0003],\n              dtype=torch.float64)                                         ,\n       ...,\n       tensor([ 0.0524,  0.0480, -0.0150,  ..., -0.0008, -0.0002,  0.0004],\n              dtype=torch.float64)                                         ,\n       tensor([ 0.1020,  0.1000,  0.2102,  ..., -0.0003, -0.0003, -0.0003],\n              dtype=torch.float64)                                         ,\n       tensor([-0.0023, -0.0014, -0.0006,  ...,  0.0003,  0.0003, -0.0006],\n              dtype=torch.float64)                                         ],\n      dtype=object)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pd['input_values'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pd = pd.read_pickle(f\"{DATA_PATH}/emotion_audio_all_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(idx):\n",
    "    idx = idx[5:7]\n",
    "    if idx == \"tr\":\n",
    "        return \"train\"\n",
    "    if idx == \"te\":\n",
    "        return \"test\"\n",
    "    else:\n",
    "        return \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pd['split'] = all_pd['path'].apply(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pd.to_pickle(f\"{DATA_PATH}/emotion_audio_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_pd['input_values'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([array([-0.14737721, -0.80998605, -0.4489204 , ...,  0.00367516,\n               0.00142903,  0.00142903], dtype=float32)               ,\n       array([ 0.01583829, -0.04533609, -0.07119843, ..., -0.00256376,\n               0.00340448,  0.00191242], dtype=float32)               ,\n       array([-0.03260687, -0.04656146, -0.04045632, ..., -0.0003369 ,\n              -0.0003369 , -0.0003369 ], dtype=float32)               ,\n       ...,\n       array([ 0.05236564,  0.04803834, -0.01501651, ..., -0.00079826,\n              -0.00018007,  0.00043811], dtype=float32)               ,\n       array([ 0.10200054,  0.10001467,  0.21023032, ..., -0.00027165,\n              -0.00027165, -0.00027165], dtype=float32)               ,\n       array([-0.00232242, -0.00143894, -0.00055545, ...,  0.00032803,\n               0.00032803, -0.00055545], dtype=float32)               ],\n      dtype=object)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jv/q0cfrvk97_z312fy17tq6l9r0000gn/T/ipykernel_57953/3242758510.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "torch.from_numpy(all_pd['input_values'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize(x):\n",
    "    return torch.tensor( x, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_indexing(v):\n",
    "    lens = np.array([len(item) for item in v])\n",
    "    mask = lens[:,None] > np.arange(lens.max())\n",
    "    out = np.zeros(mask.shape,dtype=int)\n",
    "    out[mask] = np.concatenate(v)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/Users/pranavsood/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:316184)",
      "at w.execute (/Users/pranavsood/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:315573)",
      "at w.start (/Users/pranavsood/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:311378)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/Users/pranavsood/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:325786)",
      "at async t.CellExecutionQueue.start (/Users/pranavsood/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:325326)"
     ]
    }
   ],
   "source": [
    "y = boolean_indexing(all_pd['input_values'].values)\n",
    "type(y)\n",
    "len(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13704/13704 [02:26<00:00, 93.54it/s] \n"
     ]
    }
   ],
   "source": [
    "# torch.tensor(all_pd['input_values'].values[0] , dtype=torch.float64)\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "all_pd['input_values'] = all_pd['input_values'].progress_apply(tensorize)\n",
    "all_pd['labels'] = all_pd['labels'].progress_apply(tensorize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pd.to_pickle(f\"{DATA_PATH}/emotion_audio_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1230846])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random as r\n",
    "all_pd.iloc[r.randint(0,1300)]['input_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_pd['input_values'].values[0:5])\n",
    "print(all_pd[['labels']].to_numpy()[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = all_pd['input_values'].values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([181590])\n",
      "torch.Size([181590])\n",
      "torch.Size([181590])\n",
      "torch.Size([181590])\n",
      "torch.Size([181590])\n"
     ]
    }
   ],
   "source": [
    "result = rnn_utils.pad_sequence(batch, batch_first=True)\n",
    "for tens in result:\n",
    "    print(tens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavsood/Documents/DDI/Gitlab/multi-modal-emotion/src/emotion_venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "def speech_file_to_array_fn(path , target_sampling_rate):\n",
    "    speech_array, sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "batch = [('data/train_splits_wav/dia7_utt0.wav', array(0)), ('data/train_splits_wav/dia8_utt0.wav', array(0)), ('data/train_splits_wav/dia8_utt1.wav', array(0)), ('data/train_splits_wav/dia8_utt2.wav', array(0)), ('data/train_splits_wav/dia8_utt3.wav', array(0)), ('data/train_splits_wav/dia8_utt5.wav', array(0)), ('data/train_splits_wav/dia9_utt0.wav', array(0)), ('data/train_splits_wav/dia9_utt1.wav', array(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "speech_list = []\n",
    "label_list = []\n",
    "# pdb.set_trace()\n",
    "\n",
    "\n",
    "model_path = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "\n",
    "for (input_path , label) in batch:\n",
    "    speech_list.append(speech_file_to_array_fn(\"/Users/pranavsood/Documents/DDI/Gitlab/multi-modal-emotion/\" + input_path , target_sampling_rate))\n",
    "    label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(speech_list[0]) # speech list is list of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "117760"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "m = max(map(np.shape , speech_list))[1]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(6, 46080)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = speech_list[0]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "276480"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6*46080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.resize(a , (6,m)).shape\n",
    "m = max(map(np.shape , speech_list))[1]\n",
    "lol = [   np.resize(matrix , (6,m)) for matrix in speech_list  ] # list of tensors\n",
    "# lol = [   torch.Tensor(np.resize(matrix , (6,m))) for matrix in speech_list  ] # list of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = processor(lol, sampling_rate=target_sampling_rate , padding = True)#[\"input_values\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['input_values'] # list of np array of np arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = [torch.Tensor(matrix) for matrix in result['input_values']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1,  2],\n       [ 3,  4],\n       [ 5,  6],\n       [ 7,  8],\n       [ 9, 10],\n       [11, 12]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2] , [3 , 4] , [5 , 6] , [7 , 8] , [9,10] , [11,12]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.92693849, 0.82481117, 0.921607  ],\n       [0.57440081, 0.88573636, 0.19654986],\n       [0.09443722, 0.48478423, 0.18508989],\n       [0.76760865, 0.15714948, 0.7863017 ],\n       [0.8227047 , 0.60155623, 0.44553038],\n       [0.24566853, 0.51130251, 0.55881275]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = np.random.rand(6,3)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [x , f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2, 18)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = max(map(np.shape , lst))[1]\n",
    "inp = []\n",
    "for matrix in lst:\n",
    "    n = matrix.shape[1]\n",
    "    mat = np.resize(matrix , (1,m*6))[0]\n",
    "    mat[n*6 : m*6] = 0\n",
    "    inp.append(mat)\n",
    "np.array(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(6, 2)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 1,  2],\n       [ 3,  4],\n       [ 5,  6],\n       [ 7,  8],\n       [ 9, 10],\n       [11, 12]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for mat in x:\n",
    "    print(mat.shape)\n",
    "    # mat = mat*2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,  1,  2,  3,  4,  5,\n        6])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.resize(x , (1, 18))[0]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[6*2:6*3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,  0,  0,  0,  0,  0,\n        0])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "name": "python3712jvsc74a57bd0ab1f2462a336e5b267859bcaec323d7994b03498b5a527fd46415c4b3e1c800d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}